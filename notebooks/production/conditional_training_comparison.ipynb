{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cc1903d",
   "metadata": {},
   "source": [
    "# Conditional Training A/B Comparison\n",
    "\n",
    "**Objective**: Measure whether training only on rows where `minutes > 0` improves production decision quality.\n",
    "\n",
    "**Background**: Research (Stage 6) showed that separating \"didn't play\" from \"played badly\" reduces captain regret. This notebook tests whether applying that insight to production training yields measurable benefit.\n",
    "\n",
    "**Models Compared**:\n",
    "| Model | Description |\n",
    "|-------|-------------|\n",
    "| Baseline | Production model trained on all rows |\n",
    "| Conditional | Production model trained only on `minutes > 0` |\n",
    "\n",
    "**Metrics**:\n",
    "- MAE / RMSE (sanity check)\n",
    "- Captain regret vs oracle\n",
    "- % GW with regret ≥ 10 pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f505a4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "from dugout.production.pipeline.runner import Pipeline\n",
    "from dugout.production.features.definitions import FEATURE_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dca8d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules reloaded\n"
     ]
    }
   ],
   "source": [
    "# Force reload of modules to pick up changes\n",
    "import importlib\n",
    "import dugout.production.features.builder\n",
    "import dugout.production.pipeline.runner\n",
    "importlib.reload(dugout.production.features.builder)\n",
    "importlib.reload(dugout.production.pipeline.runner)\n",
    "\n",
    "from dugout.production.pipeline.runner import Pipeline\n",
    "from dugout.production.features.definitions import FEATURE_COLUMNS\n",
    "print(\"Modules reloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be4e457",
   "metadata": {},
   "source": [
    "## 1. Train Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6d82f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BASELINE MODEL (all rows)\n",
      "============================================================\n",
      "Database: /Users/safarifgisa/Documents/Springboard/Google5DayAI/the-dugout/storage/fpl_2025_26.sqlite\n",
      "Gathered 16,559 rows, 799 players\n",
      "Built 12,620 feature rows\n",
      "Training LightGBM...\n",
      "Training residual model...\n",
      "Model saved to /Users/safarifgisa/Documents/Springboard/Google5DayAI/the-dugout/storage/production/models/lightgbm_v1/model.joblib\n"
     ]
    }
   ],
   "source": [
    "# Train baseline model (all rows)\n",
    "print(\"=\" * 60)\n",
    "print(\"BASELINE MODEL (all rows)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "baseline = Pipeline(conditional_on_play=False)\n",
    "baseline.gather_data()\n",
    "baseline.build_features()\n",
    "baseline.split()\n",
    "baseline.train()\n",
    "\n",
    "baseline_model = baseline.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95114e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CONDITIONAL MODEL (minutes > 0 only)\n",
      "============================================================\n",
      "Database: /Users/safarifgisa/Documents/Springboard/Google5DayAI/the-dugout/storage/fpl_2025_26.sqlite\n",
      "Gathered 16,559 rows, 799 players\n",
      "Built 12,620 feature rows\n",
      "Conditional training: 2,693 / 6,564 rows (minutes > 0)\n",
      "Training LightGBM...\n",
      "Training residual model...\n",
      "Model saved to /Users/safarifgisa/Documents/Springboard/Google5DayAI/the-dugout/storage/production/models/lightgbm_v1/model.joblib\n"
     ]
    }
   ],
   "source": [
    "# Train conditional model (minutes > 0 only)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CONDITIONAL MODEL (minutes > 0 only)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "conditional = Pipeline(conditional_on_play=True)\n",
    "conditional.gather_data()\n",
    "conditional.build_features()\n",
    "conditional.split()\n",
    "conditional.train()\n",
    "\n",
    "conditional_model = conditional.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da664d56",
   "metadata": {},
   "source": [
    "## 2. Evaluate MAE / RMSE (Sanity Check)\n",
    "\n",
    "Both models should be evaluated on the **same** test set (which includes all rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbd828e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Performance (all rows):\n",
      "----------------------------------------\n",
      "Baseline    : MAE=1.041, RMSE=1.948\n",
      "Conditional : MAE=1.912, RMSE=2.280\n"
     ]
    }
   ],
   "source": [
    "# Use same test set for both\n",
    "test_df = baseline.test_df.copy()\n",
    "\n",
    "# Predictions\n",
    "X_test = test_df[FEATURE_COLUMNS].values\n",
    "test_df[\"pred_baseline\"] = baseline_model.predict(X_test)\n",
    "test_df[\"pred_conditional\"] = conditional_model.predict(X_test)\n",
    "\n",
    "y_true = test_df[\"total_points\"].values\n",
    "\n",
    "results = {\n",
    "    \"Baseline\": {\n",
    "        \"MAE\": mean_absolute_error(y_true, test_df[\"pred_baseline\"]),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_true, test_df[\"pred_baseline\"])),\n",
    "    },\n",
    "    \"Conditional\": {\n",
    "        \"MAE\": mean_absolute_error(y_true, test_df[\"pred_conditional\"]),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_true, test_df[\"pred_conditional\"])),\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"\\nTest Set Performance (all rows):\")\n",
    "print(\"-\" * 40)\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name:12s}: MAE={metrics['MAE']:.3f}, RMSE={metrics['RMSE']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26beede5",
   "metadata": {},
   "source": [
    "## 3. Captain Regret Evaluation\n",
    "\n",
    "For each gameweek in the test set:\n",
    "1. Select captain as `argmax(predicted_points)` per model\n",
    "2. Find oracle captain (actual max points that GW)\n",
    "3. Regret = oracle_points - captain_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "100f914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_captain_regret(df: pd.DataFrame, pred_col: str) -> pd.DataFrame:\n",
    "    \"\"\"Compute captain regret per gameweek.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with gw, player_id, total_points, and prediction column\n",
    "        pred_col: Name of prediction column to use for captain selection\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with per-GW regret stats\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for gw in df[\"gw\"].unique():\n",
    "        gw_df = df[df[\"gw\"] == gw]\n",
    "        \n",
    "        # Oracle: player with max actual points\n",
    "        oracle_idx = gw_df[\"total_points\"].idxmax()\n",
    "        oracle_points = gw_df.loc[oracle_idx, \"total_points\"]\n",
    "        \n",
    "        # Model pick: player with max predicted points\n",
    "        captain_idx = gw_df[pred_col].idxmax()\n",
    "        captain_points = gw_df.loc[captain_idx, \"total_points\"]\n",
    "        captain_pred = gw_df.loc[captain_idx, pred_col]\n",
    "        \n",
    "        regret = oracle_points - captain_points\n",
    "        \n",
    "        results.append({\n",
    "            \"gw\": gw,\n",
    "            \"oracle_points\": oracle_points,\n",
    "            \"captain_points\": captain_points,\n",
    "            \"captain_pred\": captain_pred,\n",
    "            \"regret\": regret,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b7f4127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-GW Regret Summary:\n",
      "              mean   std  min  max\n",
      "model                             \n",
      "Baseline     13.25  1.26   12   15\n",
      "Conditional  12.25  2.50    9   15\n"
     ]
    }
   ],
   "source": [
    "# Compute regret for both models\n",
    "baseline_regret = compute_captain_regret(test_df, \"pred_baseline\")\n",
    "conditional_regret = compute_captain_regret(test_df, \"pred_conditional\")\n",
    "\n",
    "baseline_regret[\"model\"] = \"Baseline\"\n",
    "conditional_regret[\"model\"] = \"Conditional\"\n",
    "\n",
    "regret_df = pd.concat([baseline_regret, conditional_regret], ignore_index=True)\n",
    "\n",
    "print(\"Per-GW Regret Summary:\")\n",
    "print(regret_df.groupby(\"model\")[\"regret\"].agg([\"mean\", \"std\", \"min\", \"max\"]).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba3c217c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINAL COMPARISON\n",
      "======================================================================\n",
      "      Model      MAE     RMSE  Mean Regret  % GW Regret >= 10\n",
      "   Baseline 1.041177 1.947962        13.25              100.0\n",
      "Conditional 1.912234 2.280445        12.25               75.0\n"
     ]
    }
   ],
   "source": [
    "# Final comparison table\n",
    "summary = pd.DataFrame({\n",
    "    \"Model\": [\"Baseline\", \"Conditional\"],\n",
    "    \"MAE\": [results[\"Baseline\"][\"MAE\"], results[\"Conditional\"][\"MAE\"]],\n",
    "    \"RMSE\": [results[\"Baseline\"][\"RMSE\"], results[\"Conditional\"][\"RMSE\"]],\n",
    "    \"Mean Regret\": [\n",
    "        baseline_regret[\"regret\"].mean(),\n",
    "        conditional_regret[\"regret\"].mean(),\n",
    "    ],\n",
    "    \"% GW Regret >= 10\": [\n",
    "        (baseline_regret[\"regret\"] >= 10).mean() * 100,\n",
    "        (conditional_regret[\"regret\"] >= 10).mean() * 100,\n",
    "    ],\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31142824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Regret Delta: 1.00 pts/GW\n",
      "Improvement: 7.5%\n",
      "\n",
      "✅ CONCLUSION: Conditional training IMPROVES decision quality.\n",
      "   Consider making conditional_on_play=True the default.\n"
     ]
    }
   ],
   "source": [
    "# Compute delta\n",
    "delta_regret = baseline_regret[\"regret\"].mean() - conditional_regret[\"regret\"].mean()\n",
    "delta_pct = delta_regret / baseline_regret[\"regret\"].mean() * 100 if baseline_regret[\"regret\"].mean() > 0 else 0\n",
    "\n",
    "print(f\"\\nRegret Delta: {delta_regret:.2f} pts/GW\")\n",
    "print(f\"Improvement: {delta_pct:.1f}%\")\n",
    "\n",
    "if delta_regret > 0.5:\n",
    "    print(\"\\n✅ CONCLUSION: Conditional training IMPROVES decision quality.\")\n",
    "    print(\"   Consider making conditional_on_play=True the default.\")\n",
    "elif delta_regret < -0.5:\n",
    "    print(\"\\n❌ CONCLUSION: Conditional training HURTS decision quality.\")\n",
    "    print(\"   Keep baseline (all rows) as default.\")\n",
    "else:\n",
    "    print(\"\\n⚠️  CONCLUSION: Effect is NEGLIGIBLE (< 0.5 pts/GW).\")\n",
    "    print(\"   Conditional training doesn't meaningfully impact captain decisions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f6fad8",
   "metadata": {},
   "source": [
    "## 4. Captain Agreement Rate\n",
    "\n",
    "How often do both models select the same captain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffabab42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain Agreement Rate: 25.0% (1/4 GWs)\n"
     ]
    }
   ],
   "source": [
    "# Check agreement per GW\n",
    "agreement_count = 0\n",
    "total_gws = test_df[\"gw\"].nunique()\n",
    "\n",
    "for gw in test_df[\"gw\"].unique():\n",
    "    gw_df = test_df[test_df[\"gw\"] == gw]\n",
    "    baseline_pick = gw_df.loc[gw_df[\"pred_baseline\"].idxmax(), \"player_id\"]\n",
    "    conditional_pick = gw_df.loc[gw_df[\"pred_conditional\"].idxmax(), \"player_id\"]\n",
    "    if baseline_pick == conditional_pick:\n",
    "        agreement_count += 1\n",
    "\n",
    "agreement_rate = agreement_count / total_gws * 100\n",
    "print(f\"Captain Agreement Rate: {agreement_rate:.1f}% ({agreement_count}/{total_gws} GWs)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
