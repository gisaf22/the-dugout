{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "058a9c5a",
   "metadata": {},
   "source": [
    "# Stage 8b — Multi-GW Hold Decision Evaluation\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Evaluate whether **availability-adjusted cumulative EV** (`cum_ev`) outperforms **raw cumulative upside** (`cum_mu_points`) for multi-GW hold decisions.\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "Availability compounds over time. At single-GW horizons (Stages 6-7), we observed that:\n",
    "- Top players are already nailed-on starters (p_play ≈ 0.95+)\n",
    "- Availability adjustment hurt single-GW decisions\n",
    "\n",
    "However, over multiple gameweeks, availability should compound:\n",
    "- A player with p_play = 0.95 each week has only 77% chance of playing all 5 games\n",
    "- Cumulative EV should therefore protect against rotation risk\n",
    "\n",
    "## Policies\n",
    "\n",
    "| Policy | Score | Decision |\n",
    "|--------|-------|---------|\n",
    "| **cum_mu** | Σ mu_points | Select highest raw cumulative upside |\n",
    "| **cum_ev** | Σ (p_play × mu_points) | Select highest availability-adjusted cumulative EV |\n",
    "\n",
    "## Oracle\n",
    "\n",
    "For each (gw_start, horizon), the oracle is the player with highest realized total points over the horizon window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e99e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Load evaluation results\n",
    "project_root = Path.cwd().parent if \"notebooks\" in str(Path.cwd()) else Path.cwd()\n",
    "evaluation = pd.read_csv(project_root / \"storage\" / \"datasets\" / \"evaluation_multigw_hold.csv\")\n",
    "\n",
    "print(f\"Total evaluations: {len(evaluation)}\")\n",
    "evaluation.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b716e33",
   "metadata": {},
   "source": [
    "## Evaluation Metrics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9881b0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics per (policy, horizon)\n",
    "metrics = []\n",
    "\n",
    "for (policy, horizon), group in evaluation.groupby([\"policy\", \"horizon\"]):\n",
    "    regret = group[\"regret\"]\n",
    "    metrics.append({\n",
    "        \"policy\": policy,\n",
    "        \"horizon\": horizon,\n",
    "        \"mean_regret\": regret.mean(),\n",
    "        \"median_regret\": regret.median(),\n",
    "        \"pct_high_regret\": (regret >= 10).mean() * 100,\n",
    "        \"total_regret\": regret.sum(),\n",
    "        \"n_decisions\": len(regret),\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "print(\"Regret Metrics by Policy and Horizon:\\n\")\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae68fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy comparison: regret gap at each horizon\n",
    "comparison = []\n",
    "\n",
    "for horizon in sorted(evaluation[\"horizon\"].unique()):\n",
    "    cum_mu = metrics_df[(metrics_df[\"policy\"] == \"cum_mu\") & (metrics_df[\"horizon\"] == horizon)]\n",
    "    cum_ev = metrics_df[(metrics_df[\"policy\"] == \"cum_ev\") & (metrics_df[\"horizon\"] == horizon)]\n",
    "    \n",
    "    if len(cum_mu) > 0 and len(cum_ev) > 0:\n",
    "        regret_gap = cum_ev[\"mean_regret\"].values[0] - cum_mu[\"mean_regret\"].values[0]\n",
    "        winner = \"cum_ev\" if regret_gap < 0 else \"cum_mu\"\n",
    "        comparison.append({\n",
    "            \"horizon\": horizon,\n",
    "            \"cum_mu_regret\": cum_mu[\"mean_regret\"].values[0],\n",
    "            \"cum_ev_regret\": cum_ev[\"mean_regret\"].values[0],\n",
    "            \"regret_gap\": regret_gap,\n",
    "            \"winner\": winner,\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison)\n",
    "print(\"Policy Comparison (negative gap = cum_ev wins):\\n\")\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fecc0ca",
   "metadata": {},
   "source": [
    "## Plot 1: Mean Regret vs Horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5393110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Mean Regret vs Horizon\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "for policy in [\"cum_mu\", \"cum_ev\"]:\n",
    "    df = metrics_df[metrics_df[\"policy\"] == policy].sort_values(\"horizon\")\n",
    "    ax.plot(df[\"horizon\"], df[\"mean_regret\"], marker=\"o\", label=policy)\n",
    "\n",
    "ax.set_xlabel(\"Horizon (H)\")\n",
    "ax.set_ylabel(\"Mean Regret (pts)\")\n",
    "ax.set_title(\"Mean Regret vs Horizon\")\n",
    "ax.legend()\n",
    "ax.set_xticks([2, 3, 4, 5])\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb6fac6",
   "metadata": {},
   "source": [
    "## Plot 2: Tail Risk (% Regret ≥ 10) vs Horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b148dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Tail Risk vs Horizon\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "for policy in [\"cum_mu\", \"cum_ev\"]:\n",
    "    df = metrics_df[metrics_df[\"policy\"] == policy].sort_values(\"horizon\")\n",
    "    ax.plot(df[\"horizon\"], df[\"pct_high_regret\"], marker=\"o\", label=policy)\n",
    "\n",
    "ax.set_xlabel(\"Horizon (H)\")\n",
    "ax.set_ylabel(\"% Decisions with Regret ≥ 10\")\n",
    "ax.set_title(\"Tail Risk vs Horizon\")\n",
    "ax.legend()\n",
    "ax.set_xticks([2, 3, 4, 5])\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8b8ed5",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "### 1. At what horizon (if any) does `cum_ev` outperform `cum_mu_points`?\n",
    "\n",
    "Based on the evaluation results:\n",
    "- **H=2**: `cum_mu` wins by 0.43 pts mean regret\n",
    "- **H=3**: `cum_ev` wins by 0.75 pts mean regret  \n",
    "- **H=4**: `cum_ev` wins by 1.11 pts mean regret (largest gap)\n",
    "- **H=5**: `cum_mu` wins by 0.11 pts mean regret\n",
    "\n",
    "**Answer**: `cum_ev` outperforms at H=3 and H=4, but not at H=2 or H=5.\n",
    "\n",
    "### 2. How does regret gap change with horizon length?\n",
    "\n",
    "The regret gap follows a non-monotonic pattern:\n",
    "- At short horizons (H=2), availability hasn't compounded enough to matter\n",
    "- At mid horizons (H=3-4), availability compounding provides measurable protection\n",
    "- At long horizons (H=5), the advantage disappears — possibly due to increased noise in longer-term predictions or sample size effects\n",
    "\n",
    "### 3. Does availability compounding overcome upside dominance?\n",
    "\n",
    "**Partially**. Unlike single-GW decisions where `mu_points` always won:\n",
    "- At H=3 and H=4, availability adjustment provides modest improvement (~0.75-1.11 pts/decision)\n",
    "- The effect is most pronounced at H=4 where availability has had time to compound\n",
    "- However, the improvement is not consistent across all horizons\n",
    "- Tail risk (% ≥ 10 regret) is essentially identical between policies\n",
    "\n",
    "**Conclusion**: For 3-4 GW planning horizons, availability-adjusted EV shows slight improvement over raw upside. This is a reversal from single-GW findings, confirming that availability does compound over time. However, the magnitude of improvement is modest (~1 pt per decision), and the effect is not robust across all horizons."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
