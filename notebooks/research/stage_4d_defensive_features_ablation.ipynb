{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5593d8b",
   "metadata": {},
   "source": [
    "# Stage 4d: Defensive Features Ablation\n",
    "\n",
    "**Question**: Do defensive features (xGC, clean sheets, saves) improve decision quality?\n",
    "\n",
    "**Hypothesis**: Defenders and goalkeepers earn points through clean sheets and saves. Adding defensive signals may improve predictions for these positions.\n",
    "\n",
    "**Features to test**:\n",
    "- `xgc_per90`: Expected goals conceded per 90 (lower = better defense)\n",
    "- `clean_sheet_rate`: Clean sheet rate over last 5 GWs\n",
    "- `saves_per90`: Saves per 90 (GKP only, but useful)\n",
    "\n",
    "**Evaluation**: Compare captain/transfer regret with vs without these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7dd6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "from dugout.production.data.reader import DataReader\n",
    "from dugout.production.features.builder import FeatureBuilder\n",
    "from dugout.production.features.definitions import FEATURE_COLUMNS, BASE_FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79b0784",
   "metadata": {},
   "source": [
    "## 1. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50c4b35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 17,362 rows, 803 players\n",
      "GW range: 1-23\n",
      "\n",
      "Columns available: ['player_id', 'player_name', 'first_name', 'second_name', 'team_name', 'team_id', 'position', 'now_cost', 'status', 'gw', 'total_points', 'minutes', 'goals_scored', 'assists', 'clean_sheets', 'goals_conceded', 'bonus', 'bps', 'ict_index', 'influence', 'creativity', 'threat', 'xG', 'xA', 'xGI', 'xGC', 'starts', 'is_home', 'opponent_id', 'opponent_name', 'opponent_short', 'opponent_strength', 'fixture_home_goals', 'fixture_away_goals', 'fixture_finished']\n"
     ]
    }
   ],
   "source": [
    "reader = DataReader()\n",
    "raw_df = reader.get_all_gw_data()\n",
    "print(f\"Loaded {len(raw_df):,} rows, {raw_df['player_id'].nunique()} players\")\n",
    "print(f\"GW range: {raw_df['gw'].min()}-{raw_df['gw'].max()}\")\n",
    "print(f\"\\nColumns available: {list(raw_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488de492",
   "metadata": {},
   "source": [
    "## 2. Check Defensive Columns Exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f751a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ expected_goals_conceded: NOT FOUND\n",
      "✓ xGC: 17,362 non-null values, mean=0.398\n",
      "✓ clean_sheets: 17,362 non-null values, mean=0.081\n",
      "✗ saves: NOT FOUND\n",
      "✓ goals_conceded: 17,362 non-null values, mean=0.403\n"
     ]
    }
   ],
   "source": [
    "# Check for defensive columns\n",
    "defensive_cols = ['expected_goals_conceded', 'xGC', 'clean_sheets', 'saves', 'goals_conceded']\n",
    "for col in defensive_cols:\n",
    "    if col in raw_df.columns:\n",
    "        non_null = raw_df[col].notna().sum()\n",
    "        print(f\"✓ {col}: {non_null:,} non-null values, mean={raw_df[col].mean():.3f}\")\n",
    "    else:\n",
    "        print(f\"✗ {col}: NOT FOUND\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9e4814",
   "metadata": {},
   "source": [
    "## 3. Define Defensive Feature Builder (Research Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9526d71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DefensiveFeatureBuilder defined\n"
     ]
    }
   ],
   "source": [
    "class DefensiveFeatureBuilder(FeatureBuilder):\n",
    "    \"\"\"Extended feature builder with defensive metrics.\n",
    "    \n",
    "    RESEARCH ONLY - not for production use.\n",
    "    \"\"\"\n",
    "    \n",
    "    def _compute_defensive_per90(self, last5: pd.DataFrame) -> dict:\n",
    "        \"\"\"Compute defensive per90 features.\n",
    "        \n",
    "        Features:\n",
    "            - xgc_per90: Expected goals conceded per 90 (lower = better defense)\n",
    "            - clean_sheet_rate: Proportion of games with clean sheet\n",
    "            - saves_per90: Saves per 90 minutes\n",
    "        \"\"\"\n",
    "        total_mins = last5['minutes'].sum() if 'minutes' in last5.columns else 0\n",
    "        games_played = (last5['minutes'] > 0).sum() if 'minutes' in last5.columns else 0\n",
    "        \n",
    "        # Handle xGC column variants\n",
    "        xgc_col = 'xGC' if 'xGC' in last5.columns else 'expected_goals_conceded'\n",
    "        xgc_total = last5[xgc_col].sum() if xgc_col in last5.columns else 0\n",
    "        \n",
    "        def to_per90(total: float) -> float:\n",
    "            return (total / total_mins * 90) if total_mins > 0 else 0.0\n",
    "        \n",
    "        # Clean sheet rate (proportion of games with CS)\n",
    "        cs_total = last5['clean_sheets'].sum() if 'clean_sheets' in last5.columns else 0\n",
    "        cs_rate = cs_total / games_played if games_played > 0 else 0.0\n",
    "        \n",
    "        # Saves per90\n",
    "        saves_total = last5['saves'].sum() if 'saves' in last5.columns else 0\n",
    "        \n",
    "        return {\n",
    "            'xgc_per90': to_per90(xgc_total),\n",
    "            'clean_sheet_rate': cs_rate,\n",
    "            'saves_per90': to_per90(saves_total),\n",
    "        }\n",
    "    \n",
    "    def build_for_player(self, player_history: pd.DataFrame) -> dict:\n",
    "        \"\"\"Build features including defensive metrics.\"\"\"\n",
    "        # Get base features from parent\n",
    "        features = super().build_for_player(player_history)\n",
    "        \n",
    "        # Add defensive features\n",
    "        last5 = player_history.tail(5)\n",
    "        defensive = self._compute_defensive_per90(last5)\n",
    "        features.update(defensive)\n",
    "        \n",
    "        return features\n",
    "\n",
    "print(\"DefensiveFeatureBuilder defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eb0b6d",
   "metadata": {},
   "source": [
    "## 4. Build Feature Sets (Baseline vs Defensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa457e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 13,395 rows, 25 columns\n",
      "Defensive: 13,395 rows, 28 columns\n",
      "\n",
      "New defensive columns: {'clean_sheet_rate', 'saves_per90', 'xgc_per90'}\n"
     ]
    }
   ],
   "source": [
    "# Baseline features (current production)\n",
    "baseline_builder = FeatureBuilder()\n",
    "baseline_df = baseline_builder.build_training_set(raw_df)\n",
    "print(f\"Baseline: {len(baseline_df):,} rows, {len(baseline_df.columns)} columns\")\n",
    "\n",
    "# Defensive features\n",
    "defensive_builder = DefensiveFeatureBuilder()\n",
    "defensive_df = defensive_builder.build_training_set(raw_df)\n",
    "print(f\"Defensive: {len(defensive_df):,} rows, {len(defensive_df.columns)} columns\")\n",
    "\n",
    "# Show new columns\n",
    "new_cols = set(defensive_df.columns) - set(baseline_df.columns)\n",
    "print(f\"\\nNew defensive columns: {new_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ec4c88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgc_per90: mean=0.914, std=2.023, min=0.000, max=84.600\n",
      "clean_sheet_rate: mean=0.091, std=0.179, min=0.000, max=1.000\n",
      "saves_per90: mean=0.000, std=0.000, min=0.000, max=0.000\n"
     ]
    }
   ],
   "source": [
    "# Check defensive feature distributions\n",
    "for col in ['xgc_per90', 'clean_sheet_rate', 'saves_per90']:\n",
    "    if col in defensive_df.columns:\n",
    "        print(f\"{col}: mean={defensive_df[col].mean():.3f}, std={defensive_df[col].std():.3f}, \"\n",
    "              f\"min={defensive_df[col].min():.3f}, max={defensive_df[col].max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fba803c",
   "metadata": {},
   "source": [
    "## 5. Define Feature Sets for Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50c76ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline features (16): ['per90_wmean', 'per90_wvar', 'mins_mean', 'appearances', 'is_home_next', 'games_since_first', 'goals_per90', 'assists_per90', 'bonus_per90', 'bps_per90', 'ict_per90', 'xg_per90', 'xa_per90', 'ict_per90_x_mins', 'xg_per90_x_apps', 'apps_x_goals']\n",
      "\n",
      "Defensive features (18): ['per90_wmean', 'per90_wvar', 'mins_mean', 'appearances', 'is_home_next', 'games_since_first', 'goals_per90', 'assists_per90', 'bonus_per90', 'bps_per90', 'ict_per90', 'xg_per90', 'xa_per90', 'ict_per90_x_mins', 'xg_per90_x_apps', 'apps_x_goals', 'xgc_per90', 'clean_sheet_rate']\n"
     ]
    }
   ],
   "source": [
    "# Current production features\n",
    "BASELINE_FEATURES = BASE_FEATURES.copy()\n",
    "print(f\"Baseline features ({len(BASELINE_FEATURES)}): {BASELINE_FEATURES}\")\n",
    "\n",
    "# Defensive variant (excluding saves_per90 - no data available)\n",
    "DEFENSIVE_FEATURES = BASELINE_FEATURES + ['xgc_per90', 'clean_sheet_rate']\n",
    "print(f\"\\nDefensive features ({len(DEFENSIVE_FEATURES)}): {DEFENSIVE_FEATURES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11d354d",
   "metadata": {},
   "source": [
    "## 6. Train & Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17531b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def train_and_evaluate(df, feature_cols, name=\"Model\"):\n",
    "    \"\"\"Train model and return predictions.\"\"\"\n",
    "    # Split by GW (temporal split)\n",
    "    gws = sorted(df['gw'].unique())\n",
    "    test_gws = gws[-4:]  # Last 4 GWs for test\n",
    "    val_gws = gws[-8:-4]  # 4 GWs before that for val\n",
    "    train_gws = gws[:-8]  # Rest for train\n",
    "    \n",
    "    train = df[df['gw'].isin(train_gws)]\n",
    "    val = df[df['gw'].isin(val_gws)]\n",
    "    test = df[df['gw'].isin(test_gws)]\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Train: GW {min(train_gws)}-{max(train_gws)} ({len(train):,} rows)\")\n",
    "    print(f\"  Val:   GW {min(val_gws)}-{max(val_gws)} ({len(val):,} rows)\")\n",
    "    print(f\"  Test:  GW {min(test_gws)}-{max(test_gws)} ({len(test):,} rows)\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train = train[feature_cols].fillna(0)\n",
    "    y_train = train['total_points']\n",
    "    X_val = val[feature_cols].fillna(0)\n",
    "    y_val = val['total_points']\n",
    "    X_test = test[feature_cols].fillna(0)\n",
    "    y_test = test['total_points']\n",
    "    \n",
    "    # Train LightGBM\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'mae',\n",
    "        'verbosity': -1,\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.8,\n",
    "    }\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=200,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[lgb.early_stopping(20), lgb.log_evaluation(0)],\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    pred_test = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, pred_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, pred_test))\n",
    "    \n",
    "    print(f\"  Test MAE: {mae:.3f}, RMSE: {rmse:.3f}\")\n",
    "    \n",
    "    # Return test predictions for regret analysis\n",
    "    test_result = test.copy()\n",
    "    test_result['predicted_points'] = pred_test\n",
    "    \n",
    "    return model, test_result, {'mae': mae, 'rmse': rmse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3704816d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline:\n",
      "  Train: GW 6-15 (7,311 rows)\n",
      "  Val:   GW 16-19 (3,020 rows)\n",
      "  Test:  GW 20-23 (3,064 rows)\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[92]\tvalid_0's l1: 1.10175\n",
      "  Test MAE: 1.068, RMSE: 1.967\n",
      "\n",
      "Defensive:\n",
      "  Train: GW 6-15 (7,311 rows)\n",
      "  Val:   GW 16-19 (3,020 rows)\n",
      "  Test:  GW 20-23 (3,064 rows)\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's l1: 1.10351\n",
      "  Test MAE: 1.069, RMSE: 1.965\n"
     ]
    }
   ],
   "source": [
    "# Train baseline model\n",
    "baseline_model, baseline_preds, baseline_metrics = train_and_evaluate(\n",
    "    baseline_df, BASELINE_FEATURES, \"Baseline\"\n",
    ")\n",
    "\n",
    "# Train defensive model\n",
    "defensive_model, defensive_preds, defensive_metrics = train_and_evaluate(\n",
    "    defensive_df, DEFENSIVE_FEATURES, \"Defensive\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a2ff1d",
   "metadata": {},
   "source": [
    "## 7. Captain Regret Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89f6e102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain Regret Summary:\n",
      "  Baseline:  Mean=22.50, Hit Rate=0.0%\n",
      "  Defensive: Mean=12.00, Hit Rate=0.0%\n"
     ]
    }
   ],
   "source": [
    "def compute_captain_regret(predictions_df, squad_size=15):\n",
    "    \"\"\"Compute captain regret per GW.\n",
    "    \n",
    "    For each GW:\n",
    "    1. Select top N players by predicted_points (simulated squad)\n",
    "    2. Captain = argmax(predicted_points)\n",
    "    3. Oracle = actual top scorer in squad\n",
    "    4. Regret = (oracle_points - captain_points) * 2\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for gw, gw_df in predictions_df.groupby('gw'):\n",
    "        # Simulate squad selection (top N by predicted)\n",
    "        squad = gw_df.nlargest(squad_size, 'predicted_points')\n",
    "        \n",
    "        # Captain pick (argmax predicted)\n",
    "        captain = squad.loc[squad['predicted_points'].idxmax()]\n",
    "        captain_points = captain['total_points']\n",
    "        \n",
    "        # Oracle (best actual in squad)\n",
    "        oracle = squad.loc[squad['total_points'].idxmax()]\n",
    "        oracle_points = oracle['total_points']\n",
    "        \n",
    "        # Regret (doubled for captain)\n",
    "        regret = (oracle_points - captain_points) * 2\n",
    "        hit = 1 if captain['player_id'] == oracle['player_id'] else 0\n",
    "        \n",
    "        results.append({\n",
    "            'gw': gw,\n",
    "            'captain_name': captain.get('player_name', 'Unknown'),\n",
    "            'captain_points': captain_points,\n",
    "            'oracle_name': oracle.get('player_name', 'Unknown'),\n",
    "            'oracle_points': oracle_points,\n",
    "            'regret': regret,\n",
    "            'hit': hit,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Compute regret for both models\n",
    "baseline_regret = compute_captain_regret(baseline_preds)\n",
    "defensive_regret = compute_captain_regret(defensive_preds)\n",
    "\n",
    "print(\"Captain Regret Summary:\")\n",
    "print(f\"  Baseline:  Mean={baseline_regret['regret'].mean():.2f}, Hit Rate={baseline_regret['hit'].mean()*100:.1f}%\")\n",
    "print(f\"  Defensive: Mean={defensive_regret['regret'].mean():.2f}, Hit Rate={defensive_regret['hit'].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6422d667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-GW Comparison:\n",
      " gw captain_name_baseline  regret_baseline captain_name_defensive  regret_defensive  delta    winner\n",
      " 20                 Foden               14                  Foden                14      0       Tie\n",
      " 21                Gordon               26                  Foden                14     12 Defensive\n",
      " 22                 Bijol               30               E.Le Fée                 4     26 Defensive\n",
      " 23             Tavernier               20                 Thiago                16      4 Defensive\n",
      "\n",
      "Defensive wins: 3\n",
      "Baseline wins: 0\n",
      "Ties: 1\n"
     ]
    }
   ],
   "source": [
    "# Per-GW comparison\n",
    "comparison = baseline_regret[['gw', 'captain_name', 'regret']].merge(\n",
    "    defensive_regret[['gw', 'captain_name', 'regret']],\n",
    "    on='gw',\n",
    "    suffixes=('_baseline', '_defensive')\n",
    ")\n",
    "comparison['delta'] = comparison['regret_baseline'] - comparison['regret_defensive']\n",
    "comparison['winner'] = comparison['delta'].apply(\n",
    "    lambda x: 'Defensive' if x > 0 else ('Baseline' if x < 0 else 'Tie')\n",
    ")\n",
    "\n",
    "print(\"\\nPer-GW Comparison:\")\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "print(f\"\\nDefensive wins: {(comparison['winner'] == 'Defensive').sum()}\")\n",
    "print(f\"Baseline wins: {(comparison['winner'] == 'Baseline').sum()}\")\n",
    "print(f\"Ties: {(comparison['winner'] == 'Tie').sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336369a2",
   "metadata": {},
   "source": [
    "## 7b. Transfer Regret Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b1c09dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfer Regret Summary (Top-5 picks):\n",
      "  Baseline:  Mean=43.00 pts/GW\n",
      "  Defensive: Mean=46.25 pts/GW\n",
      "  Delta: -3.25 pts/GW\n"
     ]
    }
   ],
   "source": [
    "def compute_transfer_regret(predictions_df, top_k=5):\n",
    "    \"\"\"Compute transfer regret per GW.\n",
    "    \n",
    "    For each GW:\n",
    "    1. Rank all players by predicted_points\n",
    "    2. Compare model's top-K picks vs oracle's top-K actual scorers\n",
    "    3. Regret = sum(oracle_top_k_points) - sum(model_top_k_points)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for gw, gw_df in predictions_df.groupby('gw'):\n",
    "        # Model's top picks\n",
    "        model_picks = gw_df.nlargest(top_k, 'predicted_points')\n",
    "        model_points = model_picks['total_points'].sum()\n",
    "        \n",
    "        # Oracle's best picks (hindsight)\n",
    "        oracle_picks = gw_df.nlargest(top_k, 'total_points')\n",
    "        oracle_points = oracle_picks['total_points'].sum()\n",
    "        \n",
    "        # Regret\n",
    "        regret = oracle_points - model_points\n",
    "        \n",
    "        # Top pick comparison\n",
    "        model_top = model_picks.iloc[0]\n",
    "        oracle_top = oracle_picks.iloc[0]\n",
    "        \n",
    "        results.append({\n",
    "            'gw': gw,\n",
    "            'model_top': model_top.get('player_name', 'Unknown'),\n",
    "            'model_top_pts': model_top['total_points'],\n",
    "            'oracle_top': oracle_top.get('player_name', 'Unknown'),\n",
    "            'oracle_top_pts': oracle_top['total_points'],\n",
    "            'model_total': model_points,\n",
    "            'oracle_total': oracle_points,\n",
    "            'regret': regret,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Compute transfer regret for both models\n",
    "baseline_transfer = compute_transfer_regret(baseline_preds)\n",
    "defensive_transfer = compute_transfer_regret(defensive_preds)\n",
    "\n",
    "print(\"Transfer Regret Summary (Top-5 picks):\")\n",
    "print(f\"  Baseline:  Mean={baseline_transfer['regret'].mean():.2f} pts/GW\")\n",
    "print(f\"  Defensive: Mean={defensive_transfer['regret'].mean():.2f} pts/GW\")\n",
    "print(f\"  Delta: {baseline_transfer['regret'].mean() - defensive_transfer['regret'].mean():+.2f} pts/GW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f9e58b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-GW Transfer Comparison:\n",
      " gw model_top_baseline  regret_baseline model_top_defensive  regret_defensive  delta    winner\n",
      " 20              Foden               59               Foden                56      3 Defensive\n",
      " 21             Gordon               50               Foden                45      5 Defensive\n",
      " 22              Bijol               20            E.Le Fée                40    -20  Baseline\n",
      " 23          Tavernier               43              Thiago                44     -1  Baseline\n",
      "\n",
      "Defensive wins: 2\n",
      "Baseline wins: 2\n",
      "Ties: 0\n"
     ]
    }
   ],
   "source": [
    "# Per-GW transfer comparison\n",
    "transfer_comp = baseline_transfer[['gw', 'model_top', 'regret']].merge(\n",
    "    defensive_transfer[['gw', 'model_top', 'regret']],\n",
    "    on='gw',\n",
    "    suffixes=('_baseline', '_defensive')\n",
    ")\n",
    "transfer_comp['delta'] = transfer_comp['regret_baseline'] - transfer_comp['regret_defensive']\n",
    "transfer_comp['winner'] = transfer_comp['delta'].apply(\n",
    "    lambda x: 'Defensive' if x > 0 else ('Baseline' if x < 0 else 'Tie')\n",
    ")\n",
    "\n",
    "print(\"\\nPer-GW Transfer Comparison:\")\n",
    "print(transfer_comp.to_string(index=False))\n",
    "\n",
    "print(f\"\\nDefensive wins: {(transfer_comp['winner'] == 'Defensive').sum()}\")\n",
    "print(f\"Baseline wins: {(transfer_comp['winner'] == 'Baseline').sum()}\")\n",
    "print(f\"Ties: {(transfer_comp['winner'] == 'Tie').sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdff8629",
   "metadata": {},
   "source": [
    "## 8. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "392d41f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance (Defensive Model):\n",
      "          feature   importance\n",
      " ict_per90_x_mins 82385.368123\n",
      "        mins_mean 54315.099892\n",
      "        xgc_per90 15083.904095\n",
      "      per90_wmean 11045.807807\n",
      "        ict_per90  8988.462816\n",
      "         xg_per90  8882.541615\n",
      "      appearances  8517.051239\n",
      "       per90_wvar  8266.163387\n",
      "        bps_per90  8180.186914\n",
      "         xa_per90  7947.145102\n",
      "    assists_per90  6681.496105\n",
      "  xg_per90_x_apps  6247.755585\n",
      "      bonus_per90  5065.094896\n",
      " clean_sheet_rate  4094.677103\n",
      "games_since_first  3994.276903\n",
      "      goals_per90  3123.432407\n",
      "     is_home_next  2516.732714\n",
      "     apps_x_goals  1351.927213\n",
      "\n",
      "Defensive feature importance:\n",
      "  xgc_per90: 15083.9 (rank 3/18)\n",
      "  clean_sheet_rate: 4094.7 (rank 14/18)\n"
     ]
    }
   ],
   "source": [
    "# Defensive model feature importance\n",
    "importance = pd.DataFrame({\n",
    "    'feature': DEFENSIVE_FEATURES,\n",
    "    'importance': defensive_model.feature_importance('gain')\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (Defensive Model):\")\n",
    "print(importance.to_string(index=False))\n",
    "\n",
    "# Highlight defensive features\n",
    "print(\"\\nDefensive feature importance:\")\n",
    "for feat in ['xgc_per90', 'clean_sheet_rate']:\n",
    "    if feat in importance['feature'].values:\n",
    "        imp = importance[importance['feature'] == feat]['importance'].values[0]\n",
    "        rank = list(importance['feature']).index(feat) + 1\n",
    "        print(f\"  {feat}: {imp:.1f} (rank {rank}/{len(DEFENSIVE_FEATURES)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679c7ea5",
   "metadata": {},
   "source": [
    "## 9. Position-Specific Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "478b8b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline columns: ['player_id', 'player_name', 'team_name', 'team_id', 'position', 'gw', 'total_points', 'minutes']...\n",
      "Defensive columns: ['player_id', 'player_name', 'team_name', 'team_id', 'position', 'gw', 'total_points', 'minutes']...\n",
      "\n",
      "Skipping per-position analysis\n",
      "Overall MAE comparison already shown above\n"
     ]
    }
   ],
   "source": [
    "# Compare MAE by position\n",
    "def mae_by_position(preds_df, name):\n",
    "    results = []\n",
    "    if 'position' not in preds_df.columns:\n",
    "        print(f\"Warning: 'position' column not in {name}\")\n",
    "        return pd.DataFrame()\n",
    "    for pos in ['GKP', 'DEF', 'MID', 'FWD']:\n",
    "        pos_df = preds_df[preds_df['position'] == pos]\n",
    "        if len(pos_df) > 0:\n",
    "            mae = mean_absolute_error(pos_df['total_points'], pos_df['predicted_points'])\n",
    "            results.append({'position': pos, 'mae': mae, 'n': len(pos_df)})\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Check if position column exists in both\n",
    "print(f\"Baseline columns: {list(baseline_preds.columns)[:8]}...\")\n",
    "print(f\"Defensive columns: {list(defensive_preds.columns)[:8]}...\")\n",
    "\n",
    "baseline_pos = mae_by_position(baseline_preds, \"baseline\")\n",
    "defensive_pos = mae_by_position(defensive_preds, \"defensive\")\n",
    "\n",
    "if len(baseline_pos) > 0 and len(defensive_pos) > 0:\n",
    "    pos_comparison = baseline_pos.merge(defensive_pos, on='position', suffixes=('_baseline', '_defensive'))\n",
    "    pos_comparison['delta_mae'] = pos_comparison['mae_baseline'] - pos_comparison['mae_defensive']\n",
    "\n",
    "    print(\"\\nMAE by Position:\")\n",
    "    print(pos_comparison.to_string(index=False))\n",
    "    print(\"\\n(Positive delta = defensive is better)\")\n",
    "else:\n",
    "    # Just show overall MAE comparison\n",
    "    print(\"\\nSkipping per-position analysis\")\n",
    "    print(\"Overall MAE comparison already shown above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c424c3e",
   "metadata": {},
   "source": [
    "## 10. Verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aefc0325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ABLATION VERDICT: Defensive Features\n",
      "======================================================================\n",
      "\n",
      "Baseline Mean Regret:  22.50 pts/GW\n",
      "Defensive Mean Regret: 12.00 pts/GW\n",
      "Delta: +10.50 pts/GW\n",
      "\n",
      "✅ ACCEPT: Defensive features reduce regret by 10.50 pts/GW\n",
      "   Recommendation: Add to production\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "baseline_mean_regret = baseline_regret['regret'].mean()\n",
    "defensive_mean_regret = defensive_regret['regret'].mean()\n",
    "delta = baseline_mean_regret - defensive_mean_regret\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ABLATION VERDICT: Defensive Features\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nBaseline Mean Regret:  {baseline_mean_regret:.2f} pts/GW\")\n",
    "print(f\"Defensive Mean Regret: {defensive_mean_regret:.2f} pts/GW\")\n",
    "print(f\"Delta: {delta:+.2f} pts/GW\")\n",
    "\n",
    "if delta > 1.0:\n",
    "    print(f\"\\n✅ ACCEPT: Defensive features reduce regret by {delta:.2f} pts/GW\")\n",
    "    print(\"   Recommendation: Add to production\")\n",
    "elif delta < -1.0:\n",
    "    print(f\"\\n❌ REJECT: Defensive features increase regret by {-delta:.2f} pts/GW\")\n",
    "    print(\"   Recommendation: Do not add to production\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  INCONCLUSIVE: Delta ({delta:.2f}) within noise threshold (±1.0)\")\n",
    "    print(\"   Recommendation: Need more data or position-specific analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c81ef57",
   "metadata": {},
   "source": [
    "## 11. Full Walk-Forward Backtest (GW 6-23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c531616d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running walk-forward backtest (this may take a minute)...\n",
      "Baseline: Walk-forward on GW 11-23\n",
      "Defensive: Walk-forward on GW 11-23\n"
     ]
    }
   ],
   "source": [
    "def walk_forward_backtest(df, feature_cols, name=\"Model\", min_train_gws=5):\n",
    "    \"\"\"Walk-forward validation: train on GW 1..t, predict t+1.\n",
    "    \n",
    "    Returns predictions for all GWs from min_train_gws+1 onwards.\n",
    "    \"\"\"\n",
    "    gws = sorted(df['gw'].unique())\n",
    "    all_preds = []\n",
    "    \n",
    "    for i, test_gw in enumerate(gws[min_train_gws:], start=min_train_gws):\n",
    "        train_gws = gws[:i]\n",
    "        train = df[df['gw'].isin(train_gws)]\n",
    "        test = df[df['gw'] == test_gw]\n",
    "        \n",
    "        if len(test) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Train model\n",
    "        X_train = train[feature_cols].fillna(0)\n",
    "        y_train = train['total_points']\n",
    "        X_test = test[feature_cols].fillna(0)\n",
    "        \n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'mae',\n",
    "            'verbosity': -1,\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.8,\n",
    "        }\n",
    "        model = lgb.train(params, train_data, num_boost_round=100)\n",
    "        \n",
    "        # Predict\n",
    "        test_result = test.copy()\n",
    "        test_result['predicted_points'] = model.predict(X_test)\n",
    "        all_preds.append(test_result)\n",
    "    \n",
    "    print(f\"{name}: Walk-forward on GW {gws[min_train_gws]}-{gws[-1]}\")\n",
    "    return pd.concat(all_preds, ignore_index=True)\n",
    "\n",
    "# Run walk-forward for both models\n",
    "print(\"Running walk-forward backtest (this may take a minute)...\")\n",
    "baseline_wf = walk_forward_backtest(baseline_df, BASELINE_FEATURES, \"Baseline\")\n",
    "defensive_wf = walk_forward_backtest(defensive_df, DEFENSIVE_FEATURES, \"Defensive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09d88857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FULL WALK-FORWARD CAPTAIN REGRET\n",
      "======================================================================\n",
      "Baseline:  Mean=13.54 pts/GW, Hit=7.7%\n",
      "Defensive: Mean=14.46 pts/GW, Hit=30.8%\n",
      "Delta: -0.92 pts/GW\n",
      "\n",
      "======================================================================\n",
      "FULL WALK-FORWARD TRANSFER REGRET\n",
      "======================================================================\n",
      "Baseline:  Mean=51.46 pts/GW\n",
      "Defensive: Mean=54.23 pts/GW\n",
      "Delta: -2.77 pts/GW\n"
     ]
    }
   ],
   "source": [
    "# Captain regret on full backtest\n",
    "baseline_wf_regret = compute_captain_regret(baseline_wf)\n",
    "defensive_wf_regret = compute_captain_regret(defensive_wf)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FULL WALK-FORWARD CAPTAIN REGRET\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Baseline:  Mean={baseline_wf_regret['regret'].mean():.2f} pts/GW, Hit={baseline_wf_regret['hit'].mean()*100:.1f}%\")\n",
    "print(f\"Defensive: Mean={defensive_wf_regret['regret'].mean():.2f} pts/GW, Hit={defensive_wf_regret['hit'].mean()*100:.1f}%\")\n",
    "print(f\"Delta: {baseline_wf_regret['regret'].mean() - defensive_wf_regret['regret'].mean():+.2f} pts/GW\")\n",
    "\n",
    "# Transfer regret on full backtest\n",
    "baseline_wf_transfer = compute_transfer_regret(baseline_wf)\n",
    "defensive_wf_transfer = compute_transfer_regret(defensive_wf)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FULL WALK-FORWARD TRANSFER REGRET\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Baseline:  Mean={baseline_wf_transfer['regret'].mean():.2f} pts/GW\")\n",
    "print(f\"Defensive: Mean={defensive_wf_transfer['regret'].mean():.2f} pts/GW\")\n",
    "print(f\"Delta: {baseline_wf_transfer['regret'].mean() - defensive_wf_transfer['regret'].mean():+.2f} pts/GW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd78b15",
   "metadata": {},
   "source": [
    "## 12. Position-Conditional Features (xGC only for DEF/GKP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9900251e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position-conditional features applied\n",
      "Positions in data: [1 2 3 4]\n",
      "DEF/GKP count: 0\n",
      "MID/FWD count: 0\n",
      "\n",
      "DEF/GKP xgc_per90 mean: nan\n",
      "MID/FWD xgc_per90 mean: nan\n"
     ]
    }
   ],
   "source": [
    "# Position-conditional: Zero out defensive features for MID/FWD\n",
    "# Position mapping: 1=GKP, 2=DEF, 3=MID, 4=FWD\n",
    "POS_MAP = {1: 'GKP', 2: 'DEF', 3: 'MID', 4: 'FWD'}\n",
    "\n",
    "def make_position_conditional(df):\n",
    "    \"\"\"Zero out defensive features for non-defenders (MID/FWD = position 3,4).\"\"\"\n",
    "    df = df.copy()\n",
    "    for col in ['xgc_per90', 'clean_sheet_rate']:\n",
    "        if col in df.columns:\n",
    "            # Zero for MID (3) and FWD (4)\n",
    "            mask = df['position'].isin([3, 4])\n",
    "            df.loc[mask, col] = 0.0\n",
    "    return df\n",
    "\n",
    "conditional_df = make_position_conditional(defensive_df)\n",
    "print(\"Position-conditional features applied\")\n",
    "print(f\"Positions in data: {[POS_MAP.get(p, p) for p in sorted(conditional_df['position'].unique())]}\")\n",
    "\n",
    "# Show stats by position type\n",
    "def_gkp = conditional_df[conditional_df['position'].isin([1, 2])]\n",
    "mid_fwd = conditional_df[conditional_df['position'].isin([3, 4])]\n",
    "print(f\"DEF/GKP count: {len(def_gkp)}, xgc_per90 mean: {def_gkp['xgc_per90'].mean():.3f}\")\n",
    "print(f\"MID/FWD count: {len(mid_fwd)}, xgc_per90 mean: {mid_fwd['xgc_per90'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fef1045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk-forward with position-conditional features\n",
    "print(\"Running position-conditional walk-forward...\")\n",
    "conditional_wf = walk_forward_backtest(conditional_df, DEFENSIVE_FEATURES, \"Conditional\")\n",
    "\n",
    "# Captain regret\n",
    "conditional_wf_regret = compute_captain_regret(conditional_wf)\n",
    "\n",
    "# Transfer regret\n",
    "conditional_wf_transfer = compute_transfer_regret(conditional_wf)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"POSITION-CONDITIONAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nCaptain Regret:\")\n",
    "print(f\"  Baseline:    {baseline_wf_regret['regret'].mean():.2f} pts/GW\")\n",
    "print(f\"  Defensive:   {defensive_wf_regret['regret'].mean():.2f} pts/GW\")\n",
    "print(f\"  Conditional: {conditional_wf_regret['regret'].mean():.2f} pts/GW\")\n",
    "\n",
    "print(f\"\\nTransfer Regret:\")\n",
    "print(f\"  Baseline:    {baseline_wf_transfer['regret'].mean():.2f} pts/GW\")\n",
    "print(f\"  Defensive:   {defensive_wf_transfer['regret'].mean():.2f} pts/GW\")\n",
    "print(f\"  Conditional: {conditional_wf_transfer['regret'].mean():.2f} pts/GW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5536958",
   "metadata": {},
   "source": [
    "## 13. Final Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d12d963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary table\n",
    "summary = pd.DataFrame({\n",
    "    'Model': ['Baseline (16 feat)', 'Defensive (18 feat)', 'Conditional (18 feat)'],\n",
    "    'Captain Regret': [\n",
    "        baseline_wf_regret['regret'].mean(),\n",
    "        defensive_wf_regret['regret'].mean(),\n",
    "        conditional_wf_regret['regret'].mean(),\n",
    "    ],\n",
    "    'Captain Hit%': [\n",
    "        baseline_wf_regret['hit'].mean() * 100,\n",
    "        defensive_wf_regret['hit'].mean() * 100,\n",
    "        conditional_wf_regret['hit'].mean() * 100,\n",
    "    ],\n",
    "    'Transfer Regret': [\n",
    "        baseline_wf_transfer['regret'].mean(),\n",
    "        defensive_wf_transfer['regret'].mean(),\n",
    "        conditional_wf_transfer['regret'].mean(),\n",
    "    ],\n",
    "})\n",
    "\n",
    "# Highlight winners\n",
    "summary['Captain Winner'] = summary['Captain Regret'] == summary['Captain Regret'].min()\n",
    "summary['Transfer Winner'] = summary['Transfer Regret'] == summary['Transfer Regret'].min()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ABLATION SUMMARY: DEFENSIVE FEATURES\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nGWs tested: {baseline_wf['gw'].min()}-{baseline_wf['gw'].max()} (n={baseline_wf['gw'].nunique()})\")\n",
    "print(\"\\n\" + summary.to_string(index=False))\n",
    "\n",
    "# Verdict\n",
    "best_captain = summary.loc[summary['Captain Regret'].idxmin(), 'Model']\n",
    "best_transfer = summary.loc[summary['Transfer Regret'].idxmin(), 'Model']\n",
    "\n",
    "print(f\"\\n\\nVERDICT:\")\n",
    "print(f\"  Best for Captain:  {best_captain}\")\n",
    "print(f\"  Best for Transfer: {best_transfer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
